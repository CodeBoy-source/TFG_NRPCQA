\chapter*{}
%\thispagestyle{empty}
%\cleardoublepage

%\thispagestyle{empty}

% \input{portada/portada_2}


%\cleardoublepage
\thispagestyle{empty}

\begin{center}
{\large\bfseries \myTitle}\\
\end{center}
\begin{center}
\myName\\
\end{center}

%\vspace{0.7cm}
\noindent{\textbf{Palabras clave}: 
modelos 3D, nubes de puntos, imágenes biomédicas, estimación de la calidad, aprendizaje profundo, visión por computador.
}\\

\noindent{\textbf{Resumen}}\\
En el campo biomédico, la visualización y análisis de estructuras 3D
desempeña un papel fundamental. 
Sin embargo, la calidad de estas representaciones puede variar debido a diversos 
factores, como la adquisición, procesamiento y reconstrucción de las estructuras anatómicas a analizar. 
Para mejorar cada uno de estos pasos es necesario una manera de cuantificar las distorsiones que puedan surgir. 
Es decir, se hace necesaria abordar el problema que consiste en estimar la calidad 
de dichas nubes de puntos. Este campo de investigación todavía no ha sido 
suficientemente explorado en el ámbito biomédico.
Para estimar la calidad de representaciones 2D o 3D, generalmente se extraen características de la imagen original y la distorsionada para su posterior 
comparación.
No obstante, muchas veces no disponemos de la información de referencia, sino solo de la versión distorsionada.
\smallskip

Este TFG presenta un sistema capaz de estimar la calidad de 
una representación 3D biomédica sin referencia, es decir, sin emplear una imagen 
no distorsionada de referencia. La propuesta 
adapta modelos de estimación de calidad de nubes de puntos genéricas 
al ámbito médico. En concreto, se hace uso de un meta-modelo de aprendizaje profundo para 
procesar proyecciones 2D multi-vista y un vídeo del objeto 3D rotando. 
De esta forma extraemos características tanto estáticas, de imágenes 
sacadas desde una perspectiva fija, como dinámicas, de imágenes de 
perspectiva variante.
\smallskip

De cara a la validación experimental, como no existen bases de datos públicas con imágenes 
biomédicas, se emplearon nubes de puntos generalistas, de personas, animales y objetos. 
De hecho, como parte de este TFG, se propone un conjunto de datos médicos sintéticos 
generados a partir de datos privados. 
Los datos consisten en un conjunto de tomografías computerizadas y modelos 
generados mediante el escaneo láser 3D de diferentes estructuras óseas del cuerpo 
humano. A este conjunto de datos se le aplican las distorsiones más comúnes 
en las representaciones médicas 3D. 
El valor real de calidad, de cada ejemplo generado, es estimado por medio de 
los mejores métodos, para cada tipo de distorsión, según la literatura. De forma que, 
comparando la imagen distorsionada respecto a la original, 
podemos aproximar la calidad perceptual humana.
\smallskip

A pesar de que los resultados obtenidos en este trabajo no dejan de ser preliminares, 
cabe mencionar que se alcanzó una correlación entre valores de calidad obtenidos y 
deseados del 88\%, sugiriendo que se trata de una línea de investigación tremendamente 
prometedora. 

\cleardoublepage


\thispagestyle{empty}


\begin{center}
{\large\bfseries \myTitleENG}\\
\end{center}
\begin{center}
\myName \\
\end{center}

%\vspace{0.7cm}
\noindent{\textbf{Keywords}: 
3D models, point clouds, medical images, quality assessment, deep learning, computer vision.
}\\

%\vspace{0.7cm}
\noindent{\textbf{Abstract}}\\

In the biomedical field, the visualization and analysis of 3D structures play 
a fundamental role in diagnosis and research. However, the quality of these 
representations can vary due to various factors such as acquisition, processing, 
and reconstruction. To improve each of these steps, a way to quantify the distortions 
that may arise is necessary. That is, it is necessary to address the problem of 
estimating the quality of such point clouds. This research area has not been 
sufficiently explored in the biomedical domain.
\smallskip

To estimate the quality of 2D or 3D representations, features are generally extracted 
from both the original and distorted images for subsequent comparison. 
These features are derived from the analysis of structures, colors, and knowledge 
of the human visual system. However, in many cases, we only have access to the 
distorted version without the reference information.
\smallskip

This Bachelor's thesis presents a system capable of estimating the quality of a 
biomedical 3D representation without a reference, meaning it does not require an 
undistorted reference image. The proposed approach offers the possibility of adapting 
generic point cloud quality estimation models to the medical domain. Specifically, 
it leverages a meta-model based on deep learning to process multi-view 2D projections 
and a video of the 3D object rotating. By doing so, both static features from images 
taken from a fixed perspective and dynamic features from a set of perspective-varying images are extracted.
\smallskip

For experimental validation, as there are no publicly available databases with biomedical images, 
generalist point clouds of people, animals, and objects were used. 
Additionally, as part of this thesis, synthetic medical data was generated from private data, 
consisting of a collection of computed tomography scans and 3D laser-scanned models of 
different bone parts of the human body. Distortions commonly affecting medical 3D 
representations were applied to this dataset. The ground truth quality values for 
each generated example were estimated using the best reference-based methods for 
each type of distortion. By comparing the distorted images to the original ones, 
we approximate the human perceptual quality.
\smallskip

While the results obtained in this work are preliminary, it is worth mentioning 
that an 88\% correlation was achieved between the obtained quality values and the 
desired ones, suggesting that this is an extremely promising line of research.

\chapter*{}
\thispagestyle{empty}

\noindent\rule[-1ex]{\textwidth}{2pt}\\[4.5ex]

Yo, \textbf{\myName}, alumno de la titulación TITULACIÓN de la \textbf{\myFaculty}, 
con Pasaporte NX4L843F5, autorizo la ubicación de la siguiente copia de mi 
Trabajo Fin de Grado en la biblioteca del centro para que pueda ser
consultada por las personas que lo deseen.

\vspace{6cm}

\noindent Fdo: \myName

\vspace{2cm}

\begin{flushright}
Granada a \today.
\end{flushright}


\chapter*{}
\thispagestyle{empty}

\noindent\rule[-1ex]{\textwidth}{2pt}\\[4.5ex]

D. \textbf{\myProf}, Profesor del \myDepartment de la Universidad de Granada.

\vspace{0.5cm}

D. \textbf{\myOtherProf}, Profesor del \myDepartment de la Universidad de Granada.


\vspace{0.5cm}

\textbf{Informan:}

\vspace{0.5cm}

Que el presente trabajo, titulado \textit{\textbf{\myTitle}}
ha sido realizado bajo su supervisión por \textbf{\myName}, y autorizamos la defensa de dicho trabajo ante el tribunal
que corresponda.

\vspace{0.5cm}

Y para que conste, expiden y firman el presente informe en Granada a \today.

\vspace{1cm}

\textbf{Los directores:}

\vspace{5cm}

\noindent \textbf{\myProf \ \ \ \ \ \myOtherProf}

\chapter*{Agradecimientos}

\thispagestyle{empty}
       \vspace{1cm}


Primeramente, me gustaría agradecer a mis tutores, Enrique Bermejo y Pablo Mesejo,
por darme la oportunidad de desarrollar este proyecto con ellos. 
Agradezco la paciencia infinita y comprensión a la hora de resolver mis dudas. 
Segundo, agradezco a la propia Universidad de Granada por haberme dado la oportunidad 
de continuar mis estudios universitarios en tan distinguida casa de estudios. 
\\

Quiero agradecer también a mis padres, Joana Sena y Robert Netland, por la oportunidad 
de realizar mis estudios en España con todo sus apoyos. A mis compañeros de piso,
en especial al recién graduado en Física Yllari Kay, que han estado ahí desde 
primero de carrera y me han ayudado en cada paso. En general, a todos mis amigos, incluido los de Brasil,
por el cariño hacia mis obligadas ausencias. 
