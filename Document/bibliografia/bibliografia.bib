@book{VisualMedicalQualityBook,
author = {Ding, Yong},
year = {2018},
month = {03},
pages = {1-4},
title = {Visual Quality Assessment for Natural and Medical Image},
isbn = {978-3-662-56495-0},
journal = {Visual Quality Assessment for Natural and Medical Image},
doi = {10.1007/978-3-662-56497-4}
}
  
@article{MinkowskiFailure,
author = {Seshadrinathan, Kalpana and Pappas, Thrasyvoulos and Safranek, Robert and Chen, Junqing and Wang, Zhou and Sheikh, Hamid and Bovik, Alan},
year = {2009},
month = {12},
pages = {},
title = {Image Quality Assessment},
isbn = {9780123744579},
journal = {The Essential Guide to Image Processing},
doi = {10.1016/B978-0-12-374457-9.00021-4}
}

@inproceedings{Wang2006ModernIQ,
  title={Modern Image Quality Assessment},
  author={Zhou Wang and Alan Conrad Bovik},
  booktitle={Modern Image Quality Assessment},
  year={2006}
}

@Article{Parisot1995,
author={Parisot, Charles},
title={The DICOM standard},
journal={The International Journal of Cardiac Imaging},
year={1995},
month={Sep},
day={01},
volume={11},
number={3},
pages={171-177},
issn={1573-0743},
doi={10.1007/BF01143137},
url={https://doi.org/10.1007/BF01143137}
}

@article{MedicalImpactOfDistortions,
author = {Sun, Yuhao and Mogos, Gabriela},
year = {2022},
month = {03},
pages = {36-45},
title = {Impact of Visual Distortion on Medical Images},
volume = {49},
journal = {IAENG International Journal of Computer Science}
}

@article{XrayRejectionFactor,
author = {Kjelle, Elin and Chilanga, Catherine},
year = {2022},
month = {03},
pages = {},
title = {The assessment of image quality and diagnostic value in X-ray images: a survey on radiographers’ reasons for rejecting images},
volume = {13},
journal = {Insights into Imaging},
doi = {10.1186/s13244-022-01169-9}
}

@article{Slicer3D,
title = {3D Slicer as an image computing platform for the Quantitative Imaging Network},
journal = {Magnetic Resonance Imaging},
volume = {30},
number = {9},
pages = {1323-1341},
year = {2012},
note = {Quantitative Imaging in Cancer},
issn = {0730-725X},
doi = {https://doi.org/10.1016/j.mri.2012.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0730725X12001816},
author = {Andriy Fedorov and Reinhard Beichel and Jayashree Kalpathy-Cramer and Julien Finet and Jean-Christophe Fillion-Robin and Sonia Pujol and Christian Bauer and Dominique Jennings and Fiona Fennessy and Milan Sonka and John Buatti and Stephen Aylward and James V. Miller and Steve Pieper and Ron Kikinis},
keywords = {Cancer imaging, Quantitative imaging, Software tools, Medical imaging, Imaging biomarkers, Image analysis, MRI, PET, CT, Brain, Head and neck, Prostate, Glioblastima, Cancer treatment response},
abstract = {Quantitative analysis has tremendous but mostly unrealized potential in healthcare to support objective and accurate interpretation of the clinical imaging. In 2008, the National Cancer Institute began building the Quantitative Imaging Network (QIN) initiative with the goal of advancing quantitative imaging in the context of personalized therapy and evaluation of treatment response. Computerized analysis is an important component contributing to reproducibility and efficiency of the quantitative imaging techniques. The success of quantitative imaging is contingent on robust analysis methods and software tools to bring these methods from bench to bedside. 3D Slicer is a free open-source software application for medical image computing. As a clinical research tool, 3D Slicer is similar to a radiology workstation that supports versatile visualizations but also provides advanced functionality such as automated segmentation and registration for a variety of application domains. Unlike a typical radiology workstation, 3D Slicer is free and is not tied to specific hardware. As a programming platform, 3D Slicer facilitates translation and evaluation of the new quantitative methods by allowing the biomedical researcher to focus on the implementation of the algorithm and providing abstractions for the common tasks of data communication, visualization and user interface development. Compared to other tools that provide aspects of this functionality, 3D Slicer is fully open source and can be readily extended and redistributed. In addition, 3D Slicer is designed to facilitate the development of new functionality in the form of 3D Slicer extensions. In this paper, we present an overview of 3D Slicer as a platform for prototyping, development and evaluation of image analysis tools for clinical research applications. To illustrate the utility of the platform in the scope of QIN, we discuss several use cases of 3D Slicer by the existing QIN teams, and we elaborate on the future directions that can further facilitate development and validation of imaging biomarkers using 3D Slicer.}
}
@misc{SlicerWebPage,
  title = {{3D Slicer} Image Computing Plataform},
  howpublished = {\url{https://www.slicer.org/}},
  note = {Visualizada en: 2023-06-01}
}

@techreport{ITU-R.2012,
    title = {Methodology for the Subjective Assessment of the Quality of Television Pictures},
    author = {{ITU-R}},
    year = {2012},
    type = {ITU-R Recommendation},
    number = {BT.500-13},
    institution = {{International Telecommunication Union - Radiocommunication Sector (ITU-R)}},
}
@article{ITU-R.2021,
author = {Zhou, Junming and Jiang, Gangyi and Mao, Xiangying and Yu, Mei and Shao, Feng and Peng, Zongju and Zhang, Yun},
year = {2011},
month = {11},
pages = {},
title = {Subjective quality analyses of stereoscopic images in 3DTV system},
doi = {10.1109/VCIP.2011.6115913}
}

@article{DatasetGeneration,
 abstract = {Full-reference (FR) point cloud quality assessment (PCQA) has achieved impressive progress in recent years. However, in many cases, obtaining the reference point clouds is difficult, so no-reference (NR) metrics have become a research hotspot. Few researches about NR-PCQA are carried out due to the lack of a large-scale PCQA dataset. In this paper, we first build a large-scale PCQA dataset named LS-PCQA, which includes 104 reference point clouds and more than 22,000 distorted samples. In the dataset, each reference point cloud is augmented with 31 types of impairments (e.g., Gaussian noise, contrast distortion, local missing, and compression loss) at 7 distortion levels. Besides, each distorted point cloud is assigned with a pseudo quality score as its substitute of Mean Opinion Score (MOS). Inspired by the hierarchical perception system and considering the intrinsic attributes of point clouds, we propose a NR metric ResSCNN based on sparse convolutional neural network (CNN) to accurately estimate the subjective quality of point clouds. We conduct several experiments to evaluate the performance of the proposed NR metric. The results demonstrate that ResSCNN exhibits the state-of-the-art (SOTA) performance among all the existing NR-PCQA metrics and even outperforms some FR metrics. The dataset presented in this work will be made publicly accessible at http://smt.sjtu.edu.cn. The source code for the proposed ResSCNN can be found at https://github.com/lyp22/ResSCNN.},
 author = {Yipeng Liu and Qi Yang and Yiling Xu and Le Yang},
 month = {12},
 title = {Point Cloud Quality Assessment: Dataset Construction and Learning-based No-Reference Metric},
 url = {http://arxiv.org/abs/2012.11895},
 year = {2020},
}


@book{ModeloEnCascada,
    title = {Software Engineering: A Practitioner's Approach},
    author = {Pressman, R. S.},
    year = {2005},
    publisher = {Palgrave Macmillan},
}

@inproceedings{IQAbySaliencyMaps,
author = {Ardizzone, Edoardo and Bruno, Alessandro},
year = {2012},
month = {02},
pages = {},
title = {Image Quality Assessment by saliency maps},
volume = {1},
journal = {VISAPP 2012 - Proceedings of the International Conference on Computer Vision Theory and Applications}
}

@misc{StructureGuidedResampling,
      title={Blind Quality Assessment of 3D Dense Point Clouds with Structure Guided Resampling}, 
      author={Wei Zhou and Qi Yang and Qiuping Jiang and Guangtao Zhai and Weisi Lin},
      year={2022},
      eprint={2208.14603},
      archivePrefix={arXiv},
      primaryClass={cs.MM}
}

@article{StructuralSimilarityIndex,
title = {Structural similarity index (SSIM) revisited: A data-driven approach},
journal = {Expert Systems with Applications},
volume = {189},
pages = {116087},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116087},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421014238},
author = {Illya Bakurov and Marco Buzzelli and Raimondo Schettini and Mauro Castelli and Leonardo Vanneschi},
keywords = {Image quality assessment measures, Structural similarity, Evolutionary computation, Scale selection, Image processing},
abstract = {Several contemporaneous image processing and computer vision systems rely upon the full-reference image quality assessment (IQA) measures. The single-scale structural similarity index (SS-SSIM) is one of the most popular measures, and it owes its success to the mathematical simplicity, low computational complexity, and implicit incorporation of Human Visual System’s (HVS) characteristics. In this paper, we revise the original parameters of SSIM and its multi-scale counterpart (MS-SSIM) to increase their correlation with subjective evaluation. More specifically, we exploit the evolutionary computation and the swarm intelligence methods on five popular IQA databases, two of which are dedicated distance-changed databases, to determine the best combination of parameters efficiently. Simultaneously, we explore the effect of different scale selection approaches in the context of SS-SSIM. The experimental results show that with a proper fine-tuning (1) the performance of SS-SSIM and MS-SSIM can be improved, in average terms, by 8% and by 3%, respectively, (2) the SS-SSIM after the so-called standard scale selection achieves similar performance as if applying computationally more expensive state-of-the-art scale selection methods or MS-SSIM; moreover, (3) there is evidence that the parameters learned on a given database can be successfully transferred to other (previously unseen) databases; finally, (4) we propose a new set of reference parameters for SSIM’s variants and provide their interpretation.}
}

@article{CascadedIQA,
  author={Wu, Jinjian and Ma, Jupo and Liang, Fuhu and Dong, Weisheng and Shi, Guangming and Lin, Weisi},
  journal={IEEE Transactions on Image Processing}, 
  title={End-to-End Blind Image Quality Prediction With Cascaded Deep Neural Network}, 
  year={2020},
  volume={29},
  number={},
  pages={7414-7426},
  doi={10.1109/TIP.2020.3002478}
}


@inproceedings{WhyIsIQASoDifficult,
  author={Wang, Zhou and Bovik, Alan C. and Lu, Ligang},
  booktitle={2002 IEEE International Conference on Acoustics, Speech, and Signal Processing}, 
  title={Why is image quality assessment so difficult?}, 
  year={2002},
  volume={4},
  number={},
  pages={IV-3313-IV-3316},
  doi={10.1109/ICASSP.2002.5745362}
}

@article{NR3DQA,
  author={Zhang, Zicheng and Sun, Wei and Min, Xiongkuo and Wang, Tao and Lu, Wei and Zhai, Guangtao},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models}, 
  year={2022},
  volume={32},
  number={11},
  pages={7618-7631},
  doi={10.1109/TCSVT.2022.3186894}
}

@misc{VQA-PC,
      title={Treating Point Cloud as Moving Camera Videos: A No-Reference Quality Assessment Metric}, 
      author={Zicheng Zhang and Wei Sun and Yucheng Zhu and Xiongkuo Min and Wei Wu and Ying Chen and Guangtao Zhai},
      year={2022},
      eprint={2208.14085},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{GPA-NET,
      title={GPA-Net:No-Reference Point Cloud Quality Assessment with Multi-task Graph Convolutional Network}, 
      author={Ziyu Shan and Qi Yang and Rui Ye and Yujie Zhang and Yiling Xu and Xiaozhong Xu and Shan Liu},
      year={2023},
      eprint={2210.16478},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{MM-PCQA,
author = {Zhang, Zicheng and Sun, Wei and Min, Xiongkuo and Zhou, Quan and He, Jun and Wang, Qiyuan and Zhai, Guangtao},
year = {2022},
month = {09},
pages = {},
title = {MM-PCQA: Multi-Modal Learning for No-reference Point Cloud Quality Assessment},
doi = {10.48550/arXiv.2209.00244}
}

@misc{IT-PCQA,
      title={No-Reference Point Cloud Quality Assessment via Domain Adaptation}, 
      author={Qi Yang and Yipeng Liu and Siheng Chen and Yiling Xu and Jun Sun},
      year={2022},
      eprint={2112.02851},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{3DImagingInMedicine,
  title={3D imaging in medicine : algorithms, systems, applications},
  author={Karl Heinz H{\"o}hne and Henry Fuchs and Stephen M. Pizer},
  year={1990}
}
@article{3DImagingInMedicine2,
  title={Three-dimensional imaging techniques: A literature review},
  author={Orhan Hakki Karatas and Ebubekir Toy},
  journal={European Journal of Dentistry},
  year={2014},
  volume={8},
  pages={132 - 140}
}

@article{ADAS3D,
    author = {Hopman, Luuk H G A and Bhagirath, Pranav and Mulder, Mark J and Eggink, Iris N and van Rossum, Albert C and Allaart, Cornelis P and Götte, Marco J W},
    title = "{Quantification of left atrial fibrosis by 3D late gadolinium-enhanced cardiac magnetic resonance imaging in patients with atrial fibrillation: impact of different analysis methods }",
    journal = {European Heart Journal - Cardiovascular Imaging},
    volume = {23},
    number = {9},
    pages = {1182-1190},
    year = {2021},
    month = {11},
    abstract = "{Various methods and post-processing software packages have been developed to quantify left atrial (LA) fibrosis using 3D late gadolinium-enhancement cardiac magnetic resonance (LGE-CMR) images. Currently, it remains unclear how the results of these methods and software packages interrelate.Forty-seven atrial fibrillation (AF) patients underwent 3D-LGE-CMR imaging prior to their AF ablation. LA fibrotic burden was derived from the images using open-source CEMRG software and commercially available ADAS 3D-LA software. Both packages were used to calculate fibrosis based on the image intensity ratio (IIR)-method. Additionally, CEMRG was used to quantify LA fibrosis using three standard deviations (3SD) above the mean blood pool signal intensity. Intraclass correlation coefficients were calculated to compare LA fibrosis quantification methods and different post-processing software outputs. The percentage of LA fibrosis assessed using IIR threshold 1.2 was significantly different from the 3SD-method (29.80 ± 14.15\\% vs. 8.43 ± 5.42\\%; P \\&lt; 0.001). Correlation between the IIR-and SD-method was good (r = 0.85, P \\&lt; 0.001) although agreement was poor [intraclass correlation coefficient (ICC) = 0.19; P \\&lt; 0.001]. One-third of the patients were allocated to a different fibrosis category dependent on the used quantification method. Fibrosis assessment using CEMRG and ADAS 3D-LA showed good agreement for the IIR-method (ICC = 0.93; P \\&lt; 0.001).Both, the IIR1.2 and 3SD-method quantify atrial fibrotic burden based on atrial wall signal intensity differences. The discrepancy in the amount of LA fibrosis between these methods may have clinical implications when patients are classified according to their fibrotic burden. There was no difference in results between post-processing software packages to quantify LA fibrosis if an identical quantification method including the threshold was used.}",
    issn = {2047-2404},
    doi = {10.1093/ehjci/jeab245},
    url = {https://doi.org/10.1093/ehjci/jeab245},
    eprint = {https://academic.oup.com/ehjcimaging/article-pdf/23/9/1182/45316536/jeab245.pdf},
}

@article{WhatIsAI,
author = {McCarthy, John},
year = {2004},
month = {01},
pages = {},
title = {What is Artificial Intelligence?}
}

@book{LearningFromData,
added-at = {2019-10-11T10:10:38.000+0200},
author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
biburl = {https://www.bibsonomy.org/bibtex/2079c807902a5b01cf801a8c7cec519ed/lopusz_kdd},
description = {Learning From Data: Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin: 9781600490064: Amazon.com: Books},
interhash = {5665353d0134ff1dea4ae32e99335a76},
intrahash = {079c807902a5b01cf801a8c7cec519ed},
keywords = {general_machine_learning},
publisher = {AMLBook},
timestamp = {2019-10-12T23:47:07.000+0200},
title = {Learning From Data},
year = 2012
}

@incollection{TomMitchell,
  author    = {Mitchell, Tom M.},
  title     = {Machine Learning},
  booktitle = {Machine Learning},
  publisher = {McGraw-Hill},
  year      = {1997},
  chapter   = {1},
  pages     = {2},
  section   = {1.1}
}

@book{IAModernApproach,
  added-at = {2020-02-01T18:23:11.000+0100},
  author = {Russell, Stuart and Norvig, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/20533b732950d1c5ab4ac12d4f32fe637/mialhoma},
  edition = 3,
  interhash = {53908a52dd4c6c8e39f93f4ffc8341be},
  intrahash = {0533b732950d1c5ab4ac12d4f32fe637},
  keywords = {ties4530},
  publisher = {Prentice Hall},
  timestamp = {2020-02-01T18:23:11.000+0100},
  title = {Artificial Intelligence: A Modern Approach},
  year = 2010
}


@book{DataMiningHandbook,
  abstract = {Knowledge Discovery demonstrates intelligent computing at its best, and is the most desirable and interesting end-product of Information Technology. To be able to discover and to extract knowledge from data is a task that many researchers and practitioners are endeavoring to accomplish. There is a lot of hidden knowledge waiting to be discovered – this is the challenge created by today’s abundance of data.
Data Mining and Knowledge Discovery Handbook, Second Edition organizes the most current concepts, theories, standards, methodologies, trends, challenges and applications of data mining (DM) and knowledge discovery in databases (KDD) into a coherent and unified repository. This handbook first surveys, then provides comprehensive yet concise algorithmic descriptions of methods, including classic methods plus the extensions and novel methods developed recently. This volume concludes with in-depth descriptions of data mining applications in various interdisciplinary industries including finance, marketing, medicine, biology, engineering, telecommunications, software, and security.
Data Mining and Knowledge Discovery Handbook, Second Edition is designed for research scientists, libraries and advanced-level students in computer science and engineering as a reference. This handbook is also suitable for professionals in industry, for computing applications, information systems management, and strategic research management.},
  added-at = {2022-05-27T13:34:28.000+0200},
  address = {Boston, MA},
  biburl = {https://www.bibsonomy.org/bibtex/27055ea7b0321bb00dad59d9010a4f061/lepsky},
  edition = 2,
  editor = {Maimon, Oded and Rokach, Lior},
  interhash = {977ca65bc23002f7960bac0aad459876},
  intrahash = {7055ea7b0321bb00dad59d9010a4f061},
  isbn = {978-0-387-09823-4},
  keywords = {knowledge_discovery},
  publisher = {Springer Science+Business Media, LLC},
  timestamp = {2022-05-27T13:34:28.000+0200},
  title = {Data mining and knowledge discovery handbook},
  url = {https://doi.org/10.1007/978-0-387-09823-4},
  year = 2010
}

@article{ApplicationsOfIQA,
  author={Wang, Zhou},
  journal={IEEE Signal Processing Magazine}, 
  title={Applications of Objective Image Quality Assessment Methods [Applications Corner]}, 
  year={2011},
  volume={28},
  number={6},
  pages={137-142},
  doi={10.1109/MSP.2011.942295}
}

@misc{VMAF,
  author = {Netflix},
  title = {Video Multi-Method Assessment Fusion},
  year = {2016},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/charlespwd/project-title}},
}

@inproceedings{VMAFReproducibility,
author={Rassool, Reza},
booktitle={2017 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)}, 
title={VMAF reproducibility: Validating a perceptual practical video quality metric}, 
year={2017},
volume={},
number={},
pages={1-2},
doi={10.1109/BMSB.2017.7986143}
}

@inproceedings{SpatialDomainForJPEG,
author={Zhou Wang and Sheikh, H.R. and Bovik, A.C.},
  booktitle={Proceedings. International Conference on Image Processing}, 
  title={No-reference perceptual quality assessment of JPEG compressed images}, 
  year={2002},
  volume={1},
  number={},
  pages={I-I},
  doi={10.1109/ICIP.2002.1038064}
}

@inproceedings{FrecuencyDomainForJPEG,
author={Zhou Wang and Bovik, A.C. and Evan, B.L.},
  booktitle={Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)}, 
  title={Blind measurement of blocking artifacts in images}, 
  year={2000},
  volume={3},
  number={},
  pages={981-984 vol.3},
  doi={10.1109/ICIP.2000.899622}
}

@article{BIQA,
   abstract = {We propose a deep bilinear model for blind image quality assessment that works for both synthetically and authentically distorted images. Our model constitutes two streams of deep convolutional neural networks (CNNs), specializing in two distortion scenarios separately. For synthetic distortions, we first pre-train a CNN to classify the distortion type and the level of an input image, whose ground truth label is readily available at a large scale. For authentic distortions, we make use of a pre-train CNN (VGG-16) for the image classification task. The two feature sets are bilinearly pooled into one representation for a final quality prediction. We fine-tune the whole network on the target databases using a variant of stochastic gradient descent. The extensive experimental results show that the proposed model achieves state-of-the-art performance on both synthetic and authentic IQA databases. Furthermore, we verify the generalizability of our method on the large-scale Waterloo Exploration Database, and demonstrate its competitiveness using the group maximum differentiation competition methodology.},
   author = {Weixia Zhang and Kede Ma and Jia Yan and Dexiang Deng and Zhou Wang},
   doi = {10.1109/TCSVT.2018.2886771},
   issn = {15582205},
   issue = {1},
   journal = {IEEE Transactions on Circuits and Systems for Video Technology},
   keywords = {Blind image quality assessment,bilinear pooling,convolutional neural networks,gMAD competition,perceptual image processing},
   month = {1},
   pages = {36-47},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Blind Image Quality Assessment Using a Deep Bilinear Convolutional Neural Network},
   volume = {30},
   year = {2020},
}

@report{Hallucinated-IQA,
   abstract = {No-reference image quality assessment (NR-IQA) is a fundamental yet challenging task in low-level computer vision community. The difficulty is particularly pronounced for the limited information, for which the corresponding reference for comparison is typically absent. Although various feature extraction mechanisms have been leveraged from natural scene statistics to deep neural networks in previous methods, the performance bottleneck still exists. In this work, we propose a hallucination-guided quality regression network to address the issue. We firstly generate a hallucinated reference constrained on the distorted image, to compensate the absence of the true reference. Then, we pair the information of hallucinated reference with the distorted image, and forward them to the regressor to learn the perceptual discrepancy with the guidance of an implicit ranking relationship within the generator , and therefore produce the precise quality prediction. To demonstrate the effectiveness of our approach, comprehensive experiments are evaluated on four popular image quality assessment benchmarks. Our method significantly outperforms all the previous state-of-the-art methods by large margins. The code and model are publicly available on the project page https://kwanyeelin.github. io/projects/HIQA/HIQA.html.},
   author = {Kwan-Yee Lin and Guanxiang Wang},
   title = {Hallucinated-IQA: No-Reference Image Quality Assessment via Adversarial Learning},
   url = {https://kwanyeelin.github.},
}

@article{DIPIQA,
   abstract = {Objective assessment of image quality is fundamentally important in many image processing tasks. In this paper, we focus on learning blind image quality assessment (BIQA) models, which predict the quality of a digital image with no access to its original pristine-quality counterpart as reference. One of the biggest challenges in learning BIQA models is the conflict between the gigantic image space (which is in the dimension of the number of image pixels) and the extremely limited reliable ground truth data for training. Such data are typically collected via subjective testing, which is cumbersome, slow, and expensive. Here, we first show that a vast amount of reliable training data in the form of quality-discriminable image pairs (DIPs) can be obtained automatically at low cost by exploiting large-scale databases with diverse image content. We then learn an opinion-unaware BIQA (OU-BIQA, meaning that no subjective opinions are used for training) model using RankNet, a pairwise learning-to-rank (L2R) algorithm, from millions of DIPs, each associated with a perceptual uncertainty level, leading to a DIP inferred quality (dipIQ) index. Extensive experiments on four benchmark IQA databases demonstrate that dipIQ outperforms the state-of-the-art OU-BIQA models. The robustness of dipIQ is also significantly improved as confirmed by the group MAximum Differentiation competition method. Furthermore, we extend the proposed framework by learning models with ListNet (a listwise L2R algorithm) on quality-discriminable image lists (DIL). The resulting DIL inferred quality index achieves an additional performance gain.},
   author = {Kede Ma and Wentao Liu and Tongliang Liu and Zhou Wang and Dacheng Tao},
   doi = {10.1109/TIP.2017.2708503},
   issn = {10577149},
   issue = {8},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Blind image quality assessment (BIQA),RankNet,dipIQ,gMAD,learning-to-rank (L2R),quality-discriminable image pair (DIP)},
   month = {8},
   pages = {3951-3964},
   pmid = {28574353},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {DipIQ: Blind Image Quality Assessment by Learning-to-Rank Discriminable Image Pairs},
   volume = {26},
   year = {2017},
}

@book{DeepLMITPress,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@Article{DeepLearningNature,
author={LeCun, Yann
and Bengio, Yoshua
and Hinton, Geoffrey},
title={Deep learning},
journal={Nature},
year={2015},
month={May},
day={01},
volume={521},
number={7553},
pages={436-444},
abstract={Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
issn={1476-4687},
doi={10.1038/nature14539},
url={https://doi.org/10.1038/nature14539}
}
@article{DeepLearningInNN,
	doi = {10.1016/j.neunet.2014.09.003},
	url = {https://doi.org/10.1016%2Fj.neunet.2014.09.003},
	year = 2015,
	month = {jan},
	publisher = {Elsevier {BV} },
	volume = {61},
  pages = {85--117},
	author = {Jürgen Schmidhuber},
	title = {Deep learning in neural networks: An overview},
	journal = {Neural Networks}
}

@book{ANNForPattern,
  added-at = {2011-01-24T09:52:55.000+0100},
  author = {Bishop, C.M.},
  biburl = {https://www.bibsonomy.org/bibtex/2992504b34d1e4480654d4262945cb682/a.guidali},
  date-added = {2010-05-25 17:08:37 +0200},
  date-modified = {2010-05-25 17:08:37 +0200},
  interhash = {e7e5ff5e89d29b7fa2787aad12bea123},
  intrahash = {992504b34d1e4480654d4262945cb682},
  keywords = {imported},
  publisher = {Oxford University Press, USA},
  timestamp = {2011-01-24T09:52:56.000+0100},
  title = {{Neural networks for pattern recognition}},
  year = 1995
}

@book{ANNCambridge, 
  place={Cambridge},
  title={Pattern Recognition and Neural Networks},
  DOI={11.1017/CBO9780511812651},
  publisher={Cambridge University Press},
  author={Ripley, Brian D.},
  year={1996}
}

@article{NeuronImages,
author = {Meng, Zhenzhu and Hu, Yating and Ancey, Christophe},
year = {2020},
month = {02},
pages = {},
title = {Using a Data Driven Approach to Predict Waves Generated by Gravity Driven Mass Flows},
volume = {12},
journal = {Water},
doi = {10.3390/w12020600}
}

@article{NeuronSimilarity,
author = {Akgün, Ergün and Demir, Metin},
year = {2018},
month = {01},
pages = {},
title = {Modeling Course Achievements of Elementary Education Teacher Candidates with Artificial Neural Networks},
volume = {5},
journal = {International Journal of Assessment Tools in Education},
doi = {10.21449/ijate.444073}
}


@article{ShallowAndDeepNN,
author={Bakiya, A.
and Kamalanand, K.
and Rajinikanth, V.
and Nayak, Ramesh Sunder
and Kadry, Seifedine},
title={Deep neural network assisted diagnosis of time-frequency transformed electromyograms},
journal={Multimedia Tools and Applications},
year={2020},
month={Apr},
day={01},
volume={79},
number={15},
pages={11051-11068},
abstract={Electromyograms (EMG) are recorded electrical signals generated from the muscles and these signals are closely interrelated with the muscle activity and hence are useful for the investigation of neuro-muscular disorders. The feature mining, feature collection and development of classification systems are greatly significant steps in the differentiation of normal and abnormal EMG signals to evaluate the abnormality. In this work, time-frequency domain based features of regular, myopathy and Amyotrophic Lateral Sclerosis (ALS) EMG signals were extracted from four different techniques namely Stockwell-Transform (ST), Wigner-Ville Transform (WVT), Synchro-Extracting Transform (SET) and Short-Time Fourier Transform (STFT). The Particle Swarm Optimization (PSO) with fractional velocity update technique was implemented for feature reduction. Further, the classifier based on the Deep Neural Networks (DNN) was developed by employing the features selected using fractional PSO. Finally, the performance of the DNN was compared with that of the Shallow Neural Network (SNN) classifier. Results of this work demonstrate that, the performance measure of the DNN classifiers is higher than that of the SNN classifier. This work appears to be of good clinical significance since efficient classification techniques are required for the development of robust neuro-muscular diagnosis systems.},
issn={1573-7721},
doi={10.1007/s11042-018-6561-9},
url={https://doi.org/10.1007/s11042-018-6561-9}
}

@article{ConvolutionalZipCode,
  title={Backpropagation Applied to Handwritten Zip Code Recognition},
  author={Yann LeCun and Bernhard E. Boser and John S. Denker and Donnie Henderson and Richard E. Howard and Wayne E. Hubbard and Lawrence D. Jackel},
  journal={Neural Computation},
  year={1989},
  volume={1},
  pages={541-551}
}

@Article{ConvolutionInRadiology,
author={Yamashita, Rikiya
and Nishio, Mizuho
and Do, Richard Kinh Gian
and Togashi, Kaori},
title={Convolutional neural networks: an overview and application in radiology},
journal={Insights into Imaging},
year={2018},
month={Aug},
day={01},
volume={9},
number={4},
pages={611-629},
abstract={Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care.},
issn={1869-4101},
doi={10.1007/s13244-018-0639-9},
url={https://doi.org/10.1007/s13244-018-0639-9}
}

  
@article{RadiographyConvolutionExample,
author = {Rguibi, Zakaria and Hajami, Abdelmajid and Zitouni, Dya and Elqaraoui, Amine and Bedraoui, Anas},
title = {CXAI: Explaining Convolutional Neural Networks for Medical Imaging Diagnostic},
journal = {Electronics},
volume = {11},
year = {2022},
number = {11},
article-number = {1775},
url = {https://www.mdpi.com/2079-9292/11/11/1775},
issn = {2079-9292},
abstract = {Deep learning models have been increasingly applied to medical images for tasks such as lesion detection, segmentation, and diagnosis. However, the field suffers from the lack of concrete definitions for usable explanations in different settings. To identify specific aspects of explainability that may catalyse building trust in deep learning models, we will use some techniques to demonstrate many aspects of explaining convolutional neural networks in a medical imaging context. One important factor influencing clinician&rsquo;s trust is how well a model can justify its predictions or outcomes. Clinicians need understandable explanations about why a machine-learned prediction was made so they can assess whether it is accurate and clinically useful. The provision of appropriate explanations has been generally understood to be critical for establishing trust in deep learning models. However, there lacks a clear understanding on what constitutes an explanation that is both understandable and useful across different domains such as medical image analysis, which hampers efforts towards developing explanatory tool sets specifically tailored towards these tasks. In this paper, we investigated two major directions for explaining convolutional neural networks: feature-based post hoc explanatory methods that try to explain already trained and fixed target models and preliminary analysis and choice of the model architecture with an accuracy of 98% &plusmn; 0.156% from 36 CNN architectures with different configurations.},
doi = {10.3390/electronics11111775}
}

@article{ConvolutionalRepresentation,
author = {O'Shea, Keiron and Nash, Ryan},
year = {2015},
month = {11},
pages = {},
title = {An Introduction to Convolutional Neural Networks},
journal = {ArXiv e-prints}
}

@misc{SlowFastNetworks,
      title={SlowFast Networks for Video Recognition}, 
      author={Christoph Feichtenhofer and Haoqi Fan and Jitendra Malik and Kaiming He},
      year={2019},
      eprint={1812.03982},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{VoxelizationExample,
author = {Gokberk, Berk and Dutağacı, Helin and Ulaş, Aydın and Akarun, Lale and Sankur, Bulent},
year = {2008},
month = {03},
pages = {155 - 173},
title = {Representation Plurality and Fusion for 3-D Face Recognition},
volume = {38},
journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on},
doi = {10.1109/TSMCB.2007.908865}
}

@inproceedings{PointNet,
  author={Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}, 
  year={2017},
  volume={},
  number={},
  pages={77-85},
  doi={10.1109/CVPR.2017.16}}

@misc{MedicalEnsembleExample,
      title={An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks}, 
      author={Dominik Müller and Iñaki Soto-Rey and Frank Kramer},
      year={2022},
      eprint={2201.11440},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{DicomDistortionsExample,
title = {Review of medical image quality assessment},
journal = {Biomedical Signal Processing and Control},
volume = {27},
pages = {145-154},
year = {2016},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2016.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1746809416300180},
author = {Li Sze Chow and Raveendran Paramesran},
keywords = {Image Quality Assessment (IQA), No reference-IQA (NR-IQA), Medical images, MRI, CT},
abstract = {Image Quality Assessment (IQA) plays an important role in assessing any new hardware, software, image acquisition techniques, image reconstruction or post-processing algorithms, etc. In the past decade, there have been various IQA methods designed to evaluate natural images. Some were used for the medical images but the use was limited. This paper reviews the recent advancement on IQA for medical images, mainly for Magnetic Resonance Imaging (MRI), Computed Tomography (CT), and ultrasonic imaging. Thus far, there is no gold standard of IQA for medical images due to various difficulties in designing a suitable IQA for medical images, and there are many different image characteristics and contents across various imaging modalities. No reference-IQA (NR-IQA) is recommended for assessing medical images because there is no perfect reference image in the real world medical imaging. We will discuss and comment on some useful and interesting IQA methods, and then suggest several important factors to be considered in designing a new IQA method for medical images. There is still great potential for research in this area.}
}

@misc{CT2Volume,
  author = {Cairo University},
  title = {Medical Visualization and Volume Rendering},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://sbme-tutorials.github.io/2019/CG/notes/7-week7.html}},
}

@inproceedings{OctreeCompression,
author = {Schnabel, Ruwen and Klein, Reinhard},
year = {2006},
month = {01},
pages = {111-120},
title = {Octree-based Point-Cloud Compression.},
journal = {Eurographics Symposium on Point-Based Graphics},
doi = {10.2312/SPBG/SPBG06/111-120}
}
